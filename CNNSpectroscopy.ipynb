{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLAvL/OGJVL3Zdg8COMo2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qays-H/SSL-CNN/blob/main/CNNSpectroscopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import perm\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "D2G3DjkVRAjO"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_gaussian_noise_df(df, noise_std=0.01):\n",
        "    features = df.drop(columns=[\"label\"])\n",
        "    noisy_features = features + np.random.normal(0, noise_std, size=features.shape)\n",
        "    result = noisy_features.copy()\n",
        "    result[\"label\"] = 1\n",
        "    return result\n",
        "\n",
        "def add_offset_df(df):\n",
        "    features = df.drop(columns=[\"label\"])\n",
        "    training_std = features.values.std()\n",
        "    offset = np.random.uniform(-0.1, 0.1) * training_std\n",
        "    offset_features = features + offset\n",
        "    result = offset_features.copy()\n",
        "    result[\"label\"] = 2\n",
        "    return result\n",
        "\n",
        "def apply_multiplication_df(df):\n",
        "    features = df.drop(columns=[\"label\"])\n",
        "    training_std = features.values.std()\n",
        "    factor = 1 + np.random.uniform(-0.1, 0.1) * training_std\n",
        "    scaled_features = features * factor\n",
        "    result = scaled_features.copy()\n",
        "    result[\"label\"] = 3\n",
        "    return result\n",
        "\n",
        "def flip_horizontal_df(df):\n",
        "    features = df.drop(columns=[\"label\"])\n",
        "    flipped_features = features[features.columns[::-1]]\n",
        "    result = flipped_features.copy()\n",
        "    result[\"label\"] = 4\n",
        "    return result\n",
        "\n",
        "def flip_vertical_df(df):\n",
        "    features = df.drop(columns=[\"label\"])\n",
        "    mean_val = features.values.mean()\n",
        "    flipped_features = 2 * mean_val - features\n",
        "    result = flipped_features.copy()\n",
        "    result[\"label\"] = 5\n",
        "    return result\n",
        "\n",
        "def permute_segments_df(df, m=4):\n",
        "    features = df.drop(columns=[\"label\"])\n",
        "    permuted_features = []\n",
        "    for _, row in features.iterrows():\n",
        "        segments = np.array_split(row.values, m)\n",
        "        np.random.shuffle(segments)\n",
        "        permuted_row = np.concatenate(segments)\n",
        "        permuted_features.append(permuted_row)\n",
        "    permuted_df = pd.DataFrame(permuted_features, columns=features.columns)\n",
        "    permuted_df[\"label\"] = 6\n",
        "    return permuted_df"
      ],
      "metadata": {
        "id": "vwJEG0XmSj12"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SharedFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_channels=1):\n",
        "        super(SharedFeatureExtractor, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, 1, sequence_length)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = torch.flatten(x, 1)  # flatten to (batch_size, features)\n",
        "        return x\n",
        "\n",
        "class TaskHead(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(TaskHead, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 1)  # Output is a single logit for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "class MultiTaskTransformationRecognition(nn.Module):\n",
        "    def __init__(self, sequence_length):\n",
        "        super(MultiTaskTransformationRecognition, self).__init__()\n",
        "        self.feature_extractor = SharedFeatureExtractor()\n",
        "\n",
        "        # Determine the output dimension of the shared layers using a dummy input\n",
        "        dummy_input = torch.randn(1, 1, sequence_length)\n",
        "        with torch.no_grad():\n",
        "            feat_dim = self.feature_extractor(dummy_input).shape[1]\n",
        "\n",
        "        # Create 7 task-specific heads\n",
        "        self.task_heads = nn.ModuleList([TaskHead(feat_dim) for _ in range(7)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared_features = self.feature_extractor(x)  # shape: (batch_size, feature_dim)\n",
        "        outputs = [head(shared_features) for head in self.task_heads]\n",
        "        # Stack outputs into shape (batch_size, 7)\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)  # <- must be Long for CrossEntropyLoss\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class SpectralDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)  # float for BCEWithLogitsLoss\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class SpectralTransformationDataset(Dataset):\n",
        "    def __init__(self, spectra, labels):\n",
        "        self.spectra = torch.tensor(spectra, dtype=torch.float32)  # (N, 228)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)    # (N, 7)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spectra)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.spectra[idx].unsqueeze(0)\n",
        "        y = self.labels[idx]\n",
        "        return x, y\n",
        "\n",
        "class TransformationRecognitionCNN(nn.Module):\n",
        "    def __init__(self, input_length):\n",
        "        super(TransformationRecognitionCNN, self).__init__()\n",
        "\n",
        "        # Shared convolutional layers\n",
        "        self.shared_conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Calculate flattened feature size\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 1, input_length)\n",
        "            flattened_size = self.shared_conv(dummy_input).view(1, -1).shape[1]\n",
        "\n",
        "        # Seven task-specific binary classification heads\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(flattened_size, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(128, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(64, 1)  # Output a single logit per head\n",
        "            )\n",
        "            for _ in range(7)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, num_features)\n",
        "        x = x.unsqueeze(1)  # Convert to (batch_size, 1, num_features)\n",
        "        features = self.shared_conv(x)\n",
        "        features = features.view(features.size(0), -1)  # Flatten\n",
        "\n",
        "        outputs = [head(features) for head in self.heads]  # List of (batch_size, 1)\n",
        "        outputs = torch.cat(outputs, dim=1)  # Shape: (batch_size, 7)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class SpectralClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_shared_layers, input_length, num_classes):\n",
        "        super(SpectralClassifier, self).__init__()\n",
        "        self.shared_conv = pretrained_shared_layers\n",
        "\n",
        "        # Get flattened size by running dummy input through shared layers\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 1, input_length)\n",
        "            flattened_size = self.shared_conv(dummy_input).view(1, -1).shape[1]\n",
        "\n",
        "        # New classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)  # No activation, raw logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add channel dim: (batch_size, 1, input_length)\n",
        "        features = self.shared_conv(x)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        logits = self.classifier(features)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "nUkJPnmMTGwf"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_transformation_dataset(\n",
        "    raw_data, noise_data, offset_data, mult_data,\n",
        "    hflip_data, vflip_data, perm_data\n",
        "):\n",
        "    # Combine data into one array\n",
        "    all_data = np.concatenate([\n",
        "        raw_data,\n",
        "        noise_data,\n",
        "        offset_data,\n",
        "        mult_data,\n",
        "        hflip_data,\n",
        "        vflip_data,\n",
        "        perm_data,\n",
        "    ], axis=0)\n",
        "\n",
        "    num_samples = raw_data.shape[0]\n",
        "    total_samples = all_data.shape[0]\n",
        "\n",
        "    # Create one-hot labels: (total_samples, 7)\n",
        "    labels = np.zeros((total_samples, 7), dtype=np.float32)\n",
        "    for i in range(7):\n",
        "        labels[i * num_samples:(i + 1) * num_samples, i] = 1\n",
        "\n",
        "    labels = all_data.get('label')\n",
        "    # Shuffle data and labels in unison\n",
        "    indices = np.arange(total_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    all_data = all_data[indices]\n",
        "    labels = labels[indices]\n",
        "\n",
        "    return all_data, labels"
      ],
      "metadata": {
        "id": "KN9koWDGcPyU"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your CSV file\n",
        "csv_path = '/content/Dataset_RawSpectrum_NIRS_for_Intact_Mangoes.csv'  # Change this to your actual file path\n",
        "\n",
        "# Load the CSV\n",
        "# Assumes:\n",
        "# - Each row = 1 sample\n",
        "# - Each column = reflectance at one wavelength (e.g.,\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Drop non-numeric columns if necessary\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Convert to NumPy array of shape (num_samples\n",
        "raw_data = df.copy()\n",
        "raw_data['label'] = 0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5v3HkdXhcoCD"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise1 = add_gaussian_noise_df(df, noise_std=0.01)\n",
        "noise2 = add_offset_df(df)\n",
        "noise3 = apply_multiplication_df(df)\n",
        "noise4 = flip_horizontal_df(df)\n",
        "noise5 = flip_vertical_df(df)\n",
        "noise6 = permute_segments_df(df, m=4)\n",
        "\n",
        "all_data = pd.concat([raw_data,noise1,noise2,noise3,noise4,noise5,noise6])#noise1.append(noise2).append(noise3).append(noise4).append(noise5).append(noise6)\n",
        "all_data = all_data.sample(frac=1, random_state=42)\n",
        "\n",
        "X = all_data.drop(columns=['label']).values  # shape: (num_samples, num_features)\n",
        "y = pd.get_dummies(all_data['label']).values\n",
        "X_classifier = df.drop(columns=['label']).values\n",
        "y_classifier = df['label'].values"
      ],
      "metadata": {
        "id": "TmpHQGffdzlK"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_classifier_scaled = scaler.fit_transform(X_classifier)\n",
        "\n",
        "dataset = SpectralDataset(X_scaled, y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "zsFbFDPRyEnB"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_validation(model, train_loader, val_loader, num_epochs=20, lr=0.0001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # === Train ===\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)  # shape: (batch_size, 7)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "        # === Validate ===\n",
        "        model.eval()\n",
        "        correct = torch.zeros(7, device=device)\n",
        "        total = torch.zeros(7, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_X, val_y in val_loader:\n",
        "                val_X, val_y = val_X.to(device), val_y.to(device)\n",
        "                outputs = model(val_X)\n",
        "                preds = torch.sigmoid(outputs) > 0.5  # binary predictions\n",
        "\n",
        "                correct += (preds == val_y.bool()).sum(dim=0)\n",
        "                total += val_y.size(0)\n",
        "\n",
        "        per_task_acc = (correct / total).cpu().numpy()\n",
        "        acc_str = \" | \".join([f\"Task {i}: {acc:.3f}\" for i, acc in enumerate(per_task_acc)])\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Validation Accuracy per Task: {acc_str}\\n\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "t3X2XCSD1hZe"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Assuming you already have:\n",
        "#    - `X_scaled`: numpy array of shape (num_samples, num_features)\n",
        "#    - `y_onehot`: one-hot numpy array of shape (num_samples, 7)\n",
        "\n",
        "# 2. Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# 4. Create datasets and dataloaders\n",
        "train_dataset = SpectralDataset(X_train, y_train)\n",
        "val_dataset = SpectralDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CL8B4ZI_-al9"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your dataset into train/val before creating DataLoaders\n",
        "model = TransformationRecognitionCNN(input_length=X.shape[1])\n",
        "trained_model = train_model_with_validation(model, train_loader, val_loader, num_epochs=50)"
      ],
      "metadata": {
        "id": "bqDHmx3d1kSz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "input_length = X.shape[1]  # number of wavelengths\n",
        "\n",
        "# Extract pretrained shared layers\n",
        "pretrained_shared = trained_model.shared_conv\n",
        "\n",
        "# Optionally freeze them to preserve learned features\n",
        "for param in pretrained_shared.parameters():\n",
        "    param.requires_grad = False  # or True if you want to fine-tune them\n",
        "\n",
        "# Create classifier model\n",
        "classifier_model = SpectralClassifier(pretrained_shared, input_length, num_classes)"
      ],
      "metadata": {
        "id": "-U7kG8LDg8f_"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_classifier(model, train_loader, val_loader, num_epochs=20, lr=0.0001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device).long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader.dataset)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(device)\n",
        "                y_val = y_val.to(device)\n",
        "                preds = model(X_val).argmax(dim=1)\n",
        "                correct += (preds == y_val).sum().item()\n",
        "                total += y_val.size(0)\n",
        "        acc = correct / total\n",
        "        val_accuracies.append(acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Val Acc: {acc:.4f}\")\n",
        "\n",
        "    return model, train_losses, val_accuracies\n"
      ],
      "metadata": {
        "id": "BZ_Gt7BZhJq_"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_labels should be shape (num_samples,) with integers like 0, 1, 2\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_classifier_scaled, y_classifier, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = ClassificationDataset(X_train, y_train)\n",
        "val_dataset = ClassificationDataset(X_val, y_val)\n",
        "\n",
        "train_classification_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_classification_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "print(\"Unique class labels:\", np.unique(y_classifier))\n",
        "trained_classifier, train_losses, val_accuracies = train_classifier(classifier_model, train_classification_loader, val_classification_loader, num_epochs=300)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ulJmKbeOhMMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, marker='o', color='green')\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6H8CEyRkcgZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}